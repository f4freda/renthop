{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import operator\n",
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "import re\n",
    "from scipy import sparse\n",
    "from sklearn import model_selection, ensemble\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import sys\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_json(\"Downloads/Kaggle_TwoSigma_train.json\")\n",
    "test_df = pd.read_json(\"Downloads/Kaggle_TwoSigma_test.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load magic feature and merge\n",
    "photo_meta = pd.read_csv(\"Downloads/listing_image_time.csv\")\n",
    "train_df = pd.merge(train_df, photo_meta, on='listing_id', how='inner')\n",
    "test_df = pd.merge(test_df, photo_meta, on='listing_id', how='inner')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# naive features\n",
    "\n",
    "# outlier removal for price\n",
    "train_df[\"price\"] = train_df[\"price\"].clip(upper=13000)\n",
    "\n",
    "# transform price\n",
    "train_df[\"price_t\"] =train_df[\"price\"]/train_df[\"bedrooms\"]\n",
    "test_df[\"price_t\"] = test_df[\"price\"]/test_df[\"bedrooms\"] \n",
    "train_df[\"logprice\"] = np.log(train_df[\"price\"])\n",
    "test_df[\"logprice\"] = np.log(test_df[\"price\"])\n",
    "\n",
    "# count photos\n",
    "train_df[\"num_photos\"] = train_df[\"photos\"].apply(len)\n",
    "test_df[\"num_photos\"] = test_df[\"photos\"].apply(len)\n",
    "\n",
    "# price per room\n",
    "train_df[\"room_sum\"] = train_df[\"bedrooms\"]+train_df[\"bathrooms\"] \n",
    "test_df[\"room_sum\"] = test_df[\"bedrooms\"]+test_df[\"bathrooms\"] \n",
    "train_df['price_per_room'] = train_df['price']/train_df['room_sum']\n",
    "test_df['price_per_room'] = test_df['price']/test_df['room_sum']\n",
    "\n",
    "# count features\n",
    "train_df[\"num_features\"] = train_df[\"features\"].apply(len)\n",
    "test_df[\"num_features\"] = test_df[\"features\"].apply(len)\n",
    "\n",
    "# count of words present in description column #\n",
    "train_df[\"num_description_words\"] = train_df[\"description\"].apply(lambda x: len(x.split(\" \")))\n",
    "test_df[\"num_description_words\"] = test_df[\"description\"].apply(lambda x: len(x.split(\" \")))\n",
    "\n",
    "# outlier removal for number of bathrooms\n",
    "test_df[\"bathrooms\"].iloc[19671] = 2.0\n",
    "test_df[\"bathrooms\"].iloc[22977] = 2.0\n",
    "test_df[\"bathrooms\"].iloc[63719] = 2.0\n",
    "\n",
    "#created date\n",
    "train_df[\"created\"] = pd.to_datetime(train_df[\"created\"])\n",
    "test_df[\"created\"] = pd.to_datetime(test_df[\"created\"])\n",
    "train_df[\"created_month\"] = train_df[\"created\"].dt.month\n",
    "test_df[\"created_month\"] = test_df[\"created\"].dt.month\n",
    "train_df[\"created_day\"] = train_df[\"created\"].dt.day\n",
    "test_df[\"created_day\"] = test_df[\"created\"].dt.day\n",
    "train_df[\"created_hour\"] = train_df[\"created\"].dt.hour\n",
    "test_df[\"created_hour\"] = test_df[\"created\"].dt.hour\n",
    "\n",
    "# Note: tried using strftime, creating an integer field of month, date, hour. \n",
    "# Code below - commented out. Prediction was better using month, day, hour as 3 separate fields. \n",
    "# train_df[\"created2\"] = train_df[\"created\"].apply(lambda x: x.strftime('%m%d%H'))\n",
    "# test_df[\"created2\"] = test_df[\"created\"].apply(lambda x: x.strftime('%m%d%H'))\n",
    "# test_df[\"created2\"] = test_df[\"created2\"].astype(int)\n",
    "# train_df[\"created2\"] = train_df[\"created2\"].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "features_to_use=[\"bathrooms\", \"bedrooms\", \"latitude\", \n",
    "                 \"longitude\", \"price\", \"logprice\", \"price_t\", \"price_per_room\",\n",
    "                 \"num_photos\", \"num_features\", \"num_description_words\",\n",
    "                 \"listing_id\", \"created_month\", \"created_day\", \"created_hour\", \n",
    "                 \"time_stamp\"\n",
    "                ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The XGB model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below xgboost function comes from [XGB starter notebook by SRK](https://www.kaggle.com/sudalairajkumar/two-sigma-connect-rental-listing-inquiries/xgb-starter-in-python). After some tuning and based on cross validation scores, I increased the num_rounds=2000, decreased the learning rate *(param['eta'] = 0.02)*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def runXGB(train_X, train_y, test_X, test_y=None, feature_names=None, seed_val=300, num_rounds=2000):\n",
    "    \n",
    "    param = {}\n",
    "    param['objective'] = 'multi:softprob'\n",
    "    param['eta'] = 0.02\n",
    "    param['max_depth'] = 6\n",
    "    param['silent'] = 1\n",
    "    param['num_class'] = 3\n",
    "    param['eval_metric'] = \"mlogloss\"\n",
    "    param['min_child_weight'] = 1\n",
    "    param['subsample'] = 0.7\n",
    "    param['colsample_bytree'] = 0.7\n",
    "    param['seed'] = seed_val\n",
    "    num_rounds = num_rounds\n",
    "\n",
    "    plst = list(param.items())\n",
    "    xgtrain = xgb.DMatrix(train_X, label=train_y)\n",
    "\n",
    "    if test_y is not None:\n",
    "        xgtest = xgb.DMatrix(test_X, label=test_y)\n",
    "        watchlist = [(xgtrain,'train'), (xgtest, 'test')]\n",
    "        model = xgb.train(plst, xgtrain, num_rounds, watchlist, early_stopping_rounds=20)\n",
    "    else:\n",
    "        xgtest = xgb.DMatrix(test_X)\n",
    "        model = xgb.train(plst, xgtrain, num_rounds)\n",
    "\n",
    "    pred_test_y = model.predict(xgtest)\n",
    "    return pred_test_y, model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing High Cardinality Data - manager_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below treatment of manager_id came from [\"CV statistics\" notebook by @gdy5](https://www.kaggle.com/guoday/two-sigma-connect-rental-listing-inquiries/cv-statistics-better-parameters-and-explaination). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "index=list(range(train_df.shape[0]))\n",
    "random.shuffle(index)\n",
    "a=[np.nan]*len(train_df)\n",
    "b=[np.nan]*len(train_df)\n",
    "c=[np.nan]*len(train_df)\n",
    "\n",
    "for i in range(5):\n",
    "    building_level={}\n",
    "    for j in train_df['manager_id'].values:\n",
    "        building_level[j]=[0,0,0]\n",
    "    test_index=index[int((i*train_df.shape[0])/5):int(((i+1)*train_df.shape[0])/5)]\n",
    "    train_index=list(set(index).difference(test_index))\n",
    "    for j in train_index:\n",
    "        temp=train_df.iloc[j]\n",
    "        if temp['interest_level']=='low':\n",
    "            building_level[temp['manager_id']][0]+=1\n",
    "        if temp['interest_level']=='medium':\n",
    "            building_level[temp['manager_id']][1]+=1\n",
    "        if temp['interest_level']=='high':\n",
    "            building_level[temp['manager_id']][2]+=1\n",
    "    for j in test_index:\n",
    "        temp=train_df.iloc[j]\n",
    "        if sum(building_level[temp['manager_id']])!=0:\n",
    "            a[j]=building_level[temp['manager_id']][0]*1.0/sum(building_level[temp['manager_id']])\n",
    "            b[j]=building_level[temp['manager_id']][1]*1.0/sum(building_level[temp['manager_id']])\n",
    "            c[j]=building_level[temp['manager_id']][2]*1.0/sum(building_level[temp['manager_id']])\n",
    "\n",
    "train_df['manager_level_low']=a\n",
    "train_df['manager_level_medium']=b\n",
    "train_df['manager_level_high']=c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a=[]\n",
    "b=[]\n",
    "c=[]\n",
    "building_level={}\n",
    "for j in train_df['manager_id'].values:\n",
    "    building_level[j]=[0,0,0]\n",
    "for j in range(train_df.shape[0]):\n",
    "    temp=train_df.iloc[j]\n",
    "    if temp['interest_level']=='low':\n",
    "        building_level[temp['manager_id']][0]+=1\n",
    "    if temp['interest_level']=='medium':\n",
    "        building_level[temp['manager_id']][1]+=1\n",
    "    if temp['interest_level']=='high':\n",
    "        building_level[temp['manager_id']][2]+=1\n",
    "\n",
    "for i in test_df['manager_id'].values:\n",
    "    if i not in building_level.keys():\n",
    "        a.append(np.nan)\n",
    "        b.append(np.nan)\n",
    "        c.append(np.nan)\n",
    "    else:\n",
    "        a.append(building_level[i][0]*1.0/sum(building_level[i]))\n",
    "        b.append(building_level[i][1]*1.0/sum(building_level[i]))\n",
    "        c.append(building_level[i][2]*1.0/sum(building_level[i]))\n",
    "test_df['manager_level_low']=a\n",
    "test_df['manager_level_medium']=b\n",
    "test_df['manager_level_high']=c\n",
    "\n",
    "features_to_use.append('manager_level_low') \n",
    "features_to_use.append('manager_level_medium') \n",
    "features_to_use.append('manager_level_high')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Geographical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "location_dict = {\n",
    "    'cluster1': [-73.98356655,  40.75323318],\n",
    "    'cluster2': [-73.94459474,  40.82751759],\n",
    "    'cluster3': [-73.95648445,  40.77901614],\n",
    "    'cluster4': [-73.99855045,  40.71993652],\n",
    "    'cluster5': [-73.95348287,  40.688674]\n",
    "                }\n",
    "\n",
    "for location in location_dict.keys():\n",
    "    dlat = location_dict[location][0] - train_df['latitude']\n",
    "    dlon = (location_dict[location][1] - train_df['longitude']) * np.cos(np.deg2rad(41))  #  adjust for NYC latitude\n",
    "    train_df['dist_' + location] = np.sqrt(dlat ** 2 + dlon ** 2) * 60     # distance in nautical miles\n",
    "\n",
    "for location in location_dict.keys():\n",
    "    dlat = location_dict[location][0] - test_df['latitude']\n",
    "    dlon = (location_dict[location][1] - test_df['longitude']) * np.cos(np.deg2rad(41))  #  adjust for NYC latitude\n",
    "    test_df['dist_' + location] = np.sqrt(dlat ** 2 + dlon ** 2) * 60     # distance in nautical miles\n",
    "    \n",
    "features_to_use.append(\"dist_cluster1\")\n",
    "features_to_use.append(\"dist_cluster2\") \n",
    "features_to_use.append(\"dist_cluster3\") \n",
    "features_to_use.append(\"dist_cluster4\")\n",
    "features_to_use.append(\"dist_cluster5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# note: tried removing outliers \n",
    "# train_df=train_df[(train_df.longitude>train_df.longitude.quantile(0.005))\n",
    "#                   &(train_df.longitude<train_df.longitude.quantile(0.995))\n",
    "#                   &(train_df.latitude>train_df.latitude.quantile(0.005))                           \n",
    "#                   &(train_df.latitude<train_df.latitude.quantile(0.995))]\n",
    "\n",
    "train_df[\"position\"] = train_df.longitude.round(3).astype(str) + '_' + train_df.latitude.round(3).astype(str)\n",
    "test_df[\"position\"] = test_df.longitude.round(3).astype(str) + '_' + test_df.latitude.round(3).astype(str)\n",
    "\n",
    "vals = train_df[\"position\"].value_counts()\n",
    "dvals = vals.to_dict()\n",
    "train_df[\"density\"] = train_df[\"position\"].apply(lambda x: dvals.get(x, vals.min()))\n",
    "test_df[\"density\"] = test_df[\"position\"].apply(lambda x: dvals.get(x, vals.min()))\n",
    "\n",
    "features_to_use.append(\"density\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# text preprocessing for \"features\" \n",
    "\n",
    "def create_binary_features(df):\n",
    "    bows = {\n",
    "        \"pet\" : (\"dogs_allowed\", \"cats_allowed\", \"pets_ok\", \"dogs allowed\", \"cats allowed\", \"pets allowed\"),\n",
    "        \"nofee\": (\"no fee\", \"no-fee\", \"no  fee\", \"nofee\", \"no_fee\"),\n",
    "        \"lowfee\": (\"reduced_fee\", \"low_fee\", \"reduced fee\", \"low fee\"),\n",
    "        \"furnished\": (\"furnished\"),\n",
    "        \"concierge\": (\"concierge\", \"doorman\", \"housekeep\", \"in_super\"),\n",
    "        \"prewar\": (\"prewar\", \"pre_war\", \"pre war\", \"pre-war\"),\n",
    "        \"laundry\": (\"laundry\", \"lndry\"),\n",
    "        \"health\": (\"health\", \"gym\", \"fitness\", \"training\"),\n",
    "        \"transport\": (\"train\", \"subway\", \"transport\"),\n",
    "        \"parking\": (\"parking\"),\n",
    "        \"utilities\": (\"utilities\", \"heat water\", \"water included\"), \n",
    "    }\n",
    "    \n",
    "    def indicator(bow):\n",
    "        return lambda s: int(any([x in s for x in bow]))\n",
    "\n",
    "    features = df[\"features\"].apply(lambda f: \" \".join(f).lower())   # convert features to string\n",
    "    for key in bows:\n",
    "        df[\"feature_\" + key] = features.apply(indicator(bows[key]))\n",
    "\n",
    "    return df\n",
    "\n",
    "create_binary_features(train_df)\n",
    "create_binary_features(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# feature engineering on \"description\"\n",
    "\n",
    "def cap_share(x):\n",
    "    return sum(1 for c in x if c.isupper())/float(len(x)+1)\n",
    "\n",
    "for df in [train_df, test_df]:\n",
    "    #ALL CAPS description\n",
    "    df['num_caps'] = df['description'].apply(cap_share)\n",
    "    \n",
    "    # length of description in lines shown\n",
    "    df['num_lines'] = df['description'].apply(lambda x: x.count('<br /><br />'))\n",
    "    \n",
    "    # Email address included\n",
    "    df['num_email'] = 0\n",
    "    df['num_email'].ix[df['description'].str.contains('@')] = 1\n",
    "    \n",
    "    # Phone number included\n",
    "    reg = re.compile(\".*?(\\(?\\d{3}\\D{0,3}\\d{3}\\D{0,3}\\d{4}).*?\", re.S)\n",
    "\n",
    "def try_and_find_nr(description):\n",
    "    if reg.match(description) is None:\n",
    "        return 0\n",
    "    return 1\n",
    "\n",
    "train_df[\"num_phone_nr\"] = train_df['description'].apply(try_and_find_nr)\n",
    "test_df[\"num_phone_nr\"] = test_df['description'].apply(try_and_find_nr)\n",
    "\n",
    "features_to_use.append(\"num_caps\")\n",
    "features_to_use.append(\"num_lines\")\n",
    "features_to_use.append(\"num_phone_nr\")\n",
    "features_to_use.append(\"num_email\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# transforming categorical variables into numerical continuous variables. plain old LabelEncoder.\n",
    "# I tried get_dummies / OneHotEncoder but dimensions for \"display_address\" got out of control. \n",
    "# In retrospect, more text cleanup was needed to link duplicate addresses. \n",
    "\n",
    "categorical = [\"display_address\", \"manager_id\", \"building_id\"]\n",
    "\n",
    "for f in categorical:\n",
    "        if train_df[f].dtype=='object':\n",
    "            #print(f)\n",
    "            lbl = LabelEncoder()\n",
    "            lbl.fit(list(train_df[f].values) + list(test_df[f].values))\n",
    "            train_df[f] = lbl.transform(list(train_df[f].values))\n",
    "            test_df[f] = lbl.transform(list(test_df[f].values))\n",
    "            features_to_use.append(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0                                                     \n",
      "1    Doorman Elevator Fitness_Center Cats_Allowed D...\n",
      "2    Laundry_In_Building Dishwasher Hardwood_Floors...\n",
      "3                               Hardwood_Floors No_Fee\n",
      "4                                              Pre-War\n",
      "Name: features, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# CountVectorizer and tdidf for \"features\"\n",
    "\n",
    "train_df['features'] = train_df[\"features\"].apply(lambda x: \" \".join([\"_\".join(i.split(\" \")) for i in x]))\n",
    "test_df['features'] = test_df[\"features\"].apply(lambda x: \" \".join([\"_\".join(i.split(\" \")) for i in x]))\n",
    "print(train_df[\"features\"].head())\n",
    "tfidf = CountVectorizer(stop_words='english', max_features=200)\n",
    "tr_sparse = tfidf.fit_transform(train_df[\"features\"])\n",
    "te_sparse = tfidf.transform(test_df[\"features\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((49352, 242), (74659, 242))\n"
     ]
    }
   ],
   "source": [
    "train_X = sparse.hstack([train_df[features_to_use], tr_sparse]).tocsr()\n",
    "test_X = sparse.hstack([test_df[features_to_use], te_sparse]).tocsr()\n",
    "\n",
    "target_num_map = {'high':0, 'medium':1, 'low':2}\n",
    "train_y = np.array(train_df['interest_level'].apply(lambda x: target_num_map[x]))\n",
    "print(train_X.shape, test_X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Note on XGBoost: NaN and infinity are treated as specific values, accepted as input.\n",
    "# As a result, there was no need to fillna() or replace() inf in this script. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cv_scores = []\n",
    "kf = model_selection.KFold(n_splits=5, shuffle=True, random_state=300)\n",
    "for dev_index, val_index in kf.split(range(train_X.shape[0])):\n",
    "        dev_X, val_X = train_X[dev_index,:], train_X[val_index,:]\n",
    "        dev_y, val_y = train_y[dev_index], train_y[val_index]\n",
    "        preds, model = runXGB(dev_X, dev_y, val_X, val_y, num_rounds = 300) #change num_rounds for shorter time in cv\n",
    "        cv_scores.append(log_loss(val_y, preds))\n",
    "        print(cv_scores)\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run model and save predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "preds, model = runXGB(train_X, train_y, test_X, num_rounds=2000)\n",
    "out_df = pd.DataFrame(preds)\n",
    "out_df.columns = [\"high\", \"medium\", \"low\"]\n",
    "out_df[\"listing_id\"] = test_df.listing_id.values\n",
    "out_df.to_csv(\"predictions.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
